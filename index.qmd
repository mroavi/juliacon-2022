---
title: JunctionTrees.jl
subtitle: Efficient Bayesian Inference in Probabilistic Graphical Models
#subtitle: Efficient Bayesian Inference in Discrete Graphical Models
#subtitle: Efficient Inference in Bayesian Networks
#subtitle: Efficient Bayesian Inference in Discrete Graphical Models
author: Martin Roa Villescas
#date: "27/07/2022"
#institute: JuliaCon 2022
#title-slide-attributes:
  #data-background-image: figures/junction-trees-logo-short.png
  #data-background-size: contain  
execute:
  freeze: auto
format:
  revealjs:
    theme: mytheme.scss
    center: true
    incremental: true   
    code-fold: false
    logo: figures/tue-logo-short.svg
    auto-stretch: false
    echo: true
    slide-number: true
    transition: none
jupyter: julia-1.7
---

## {.smaller}

::: {.r-fit-text}
**Motivating example**: patient diagnosis
:::

:::: {.columns}

::: {.column width="30%"}

![](figures/dyspnoea.jpg){width=100% fig-align="center"}

:::

::: {.column width="70%" .fragment}

<blockquote>
Shortness-of-breath (**dyspnoea**) may be due to **tuberculosis**, **lung
cancer** or **bronchitis**, or none of them, or more than one of them. A
**recent visit to Asia** increases the chances of **tuberculosis**, while
**smoking** is known to be a risk factor for both **lung cancer** and
**bronchitis**. The results of a single chest **X-ray** do not discriminate
between **lung cancer** and **tuberculosis**, as neither does the presence or
absence of **dyspnoea**.
</blockquote>
\

::: {.fragment}
**Proposed solution**: a Bayesian approach
:::

:::

::::

::: footer
*Local Computations with Probabilities on Graphical Structures and their Application to Expert Systems* by S. L. LAURITZEN and D. J. SPIEGELHALTER
:::


## The ASIA network {.smaller}

:::: {.columns}

::: {.column width="60%"}

| **Random variable**  | **Meaning**                     |
|        :---:         | :---                            |
|        ``A``         | Recent trip to Asia             |
|        ``T``         | Patient has tuberculosis        |
|        ``S``         | Patient is a smoker             |
|        ``L``         | Patient has lung cancer         |
|        ``B``         | Patient has bronchitis          |
|        ``E``         | Patient has ``T`` and/or ``L``  |
|        ``X``         | Chest X-Ray is positive         |
|        ``D``         | Patient has dyspnoea            |

:::

::: {.column width="40%"}

::: {.fragment}

```{=tex}
\begin{gather*}
  P({\bf V}) = \prod_{V \in {\bf V}} P(V \mid parents(V))
\end{gather*}
```

:::

![](figures/bayesian-network/output-1.svg){width=100% fig-align="center" .fragment}

:::

::::


## The ASIA network {.smaller}

:::: {.columns}

::: {.column width="60%"}

| **Random variable**  | **Meaning**                     |
|        :---:         | :---                            |
|        ``A``         | Recent trip to Asia             |
|        ``T``         | Patient has tuberculosis        |
|        ``S``         | Patient is a smoker             |
|        ``L``         | Patient has lung cancer         |
|        ``B``         | Patient has bronchitis          |
|        ``E``         | Patient has ``T`` and/or ``L``  |
|        ``X``         | Chest X-Ray is positive         |
|        ``D``         | Patient has dyspnoea            |

:::

::: {.column width="40%"}

```{=tex}
\begin{gather*}
  P({\bf V}) = \prod_{V \in {\bf V}} P(V \mid parents(V))
\end{gather*}
```

![](figures/bayesian-network/output-2.svg){width=100% fig-align="center"}

:::

::::


## Problem statement {.smaller}

1. Given that a patient has **dyspnoea** and has **recently visited Asia**,
calculate the probability that this person has **bronchitis**, **lung cancer**
or **tuberculosis**.

2. More generally, given a **set of random variables** and their **joint distribution**,
compute one or more **conditional distributions** given **observations**.

3. Do this **efficiently** for practical (large) models.

:::: {.columns .fragment}

::: {.column width="70%"}
::: {.r-fit-text}
**Proposed solution**: JunctionTrees.jl 
:::
:::

::: {.column width="30%"}
![](figures/junction-trees-logo-short.png){width=5.1% .absolute top=345 left=750}
:::

::::


## Outline {visibility="hidden"}

::: {.nonincremental}

- The inference problem
- The junction tree algorithm
- JunctionTrees.jl ![](figures/junction-trees-logo-short.png){width=5.1% .absolute top=235 left=320}
:::


# The inference problem {visibility="hidden"}


## The inference problem {visibility="hidden"}

::: {.notes}
- The central problem of computational Probability Theory is the inference
problem:
    - Given a set of random variables X1, . . . , Xk and their joint density,
    compute one or more conditional densities given observations.
:::

<blockquote cite="https://ai.stanford.edu/~paskin/gm-short-course/lec1.pdf">
*Given a set of **random variables** $\mathcal{V}$ and their **joint distribution** $P(\mathcal{V})$, compute one or more conditional distributions given observations.*
</blockquote>

::: footer
*A Short Course on Graphical Models* by M. A. Paskin (2003).
:::


## The inference problem {visibility="hidden"}

![](figures/the-inference-problem/output.svg){width=100% fig-align="center"}


# The junction tree algorithm

::: {.r-fit-text}
An efficient method to perform *Bayesian inference* in general graphs.
:::

::: {.notes}
Add reference to the paper.
Explain the idea of the Junction algorithm.
Explain that it is a different representatin of the model that allows calculating probabilistic queries more efficiently.
:::

::: footer
*Inference in Belief Networks: A Procedural Guide* by C. HUANG and A. DARWICHE (1996).
:::

## Overview

![](figures/jta-overview/output-1-0.svg){width=55% fig-align="center"}


## Overview

![](figures/jta-overview/output-1-1.svg){width=55% fig-align="center"}


## Bayesian network {auto-animate=true}

![](figures/bayesian-network/output-1.svg){width=60% fig-align="center"}


## Moralization {auto-animate=true}

::: {.notes}
Marry the parents of each variable and drop the directions of the edges.
:::

:::: {.columns}

::: {.column width="50%"}

![](figures/bayesian-network/output-1.svg){width=80% fig-align="center"}

:::

::: {.column width="50%" }

![](figures/moral-graph/output.svg){width=80% fig-align="center"}

:::

::::

![](figures/right-arrow-unfilled.svg){width=5% .absolute top=200 left=490}


## Moral graph {auto-animate=true}

![](figures/moral-graph/output.svg){width=60% fig-align="center"}


## Triangulation {auto-animate=true}

::: {.notes}
- Consists of removing every cycle of length greater than three in a graph.
- We do so by connecting two nonadjacent nodes in every cycle of length $>$ three.
- An optimal triangulation minimizes the sum of the state space sizes of the cliques.
- This is equivalent to minimizing the size of the largest clique.
- This problem is *NP-complete*.

- This is the hardest of all steps. And when I say hard, I mean it literally.
This problem is NP-hard.
- The goal is to get rid of all cycles of length greater than 3!
- Note that a byproduct of triangulation is the set of maximal cliques of the
triangulated graph, which are extremely useful for the junction algorithm:
these maximal cliques become the nodes of the final junction tree.
- The overall goal is to minimize the state space size of the system.
- This is equivalent to minimizing the number of vars in the largest
clique/cluster.
- The enclosed sets of nodes are maximal cliques which, in this case, become
the clusters of the junction tree.
- A clique is a set of variables all connected to each other.
other.
- A maximal clique is a one that is not contained in a larger one.
:::

:::: {.columns}

::: {.column width="50%"}

![](figures/moral-graph/output.svg){width=80% fig-align="center"}

:::

::: {.column width="50%"}

![](figures/triangulated-graph/output-1.svg){width=80% fig-align="center"}

:::

::::

![](figures/right-arrow-unfilled.svg){width=5% .absolute top=200 left=490}

## Triangulated graph {auto-animate=true}

![](figures/triangulated-graph/output-1.svg){width=60% fig-align="center"}


## Maximal cliques {auto-animate=true}

:::: {.columns}

::: {.column width="50%"}

![](figures/triangulated-graph/output-1.svg){width=80% fig-align="center"}

:::

::: {.column width="50%"}

![](figures/triangulated-graph/output-2.svg){width=80% fig-align="center"}

:::

::::

![](figures/right-arrow-unfilled.svg){width=5% .absolute top=200 left=490}


## Maximal cliques {auto-animate=true}

![](figures/triangulated-graph/output-2.svg){width=60% fig-align="center"}


## Connection of clusters {auto-animate=true}

::: {.notes}
Consists of transforming the triangulated graph into a *junction tree*.
A junction tree is a tree that satisfies the *running intersection
property*: *All clusters on the path between two given clusters contain their
common variables.*
The maximal cliques of the triangulated graphs correspond to the nodes of the
junction tree.
A clique in an undirected graph is a subgraph in which an edge connects every
pair of nodes. A maximal clique is a clique that is not contained in a larger
clique.
:::

:::: {.columns}

::: {.column width="50%"}

![](figures/triangulated-graph/output-2.svg){width=80% fig-align="center"}

:::

::: {.column width="50%"}

![](figures/junction-tree/output-1.svg){width=80% fig-align="center"}

:::

::::

![](figures/right-arrow-unfilled.svg){width=5% .absolute top=200 left=490}

::: footer
*Optimal junction trees* by Finn V. JENSEN and Frank JENSEN (1994).
:::


## Junction tree {auto-animate=true}

![](figures/junction-tree/output-1.svg){width=60% fig-align="center"}


## Overview

![](figures/jta-overview/output-1-2.svg){width=55% fig-align="center"}


## Initialization {.smaller}

:::: {.columns}

::: {.column width="50%"}

![](figures/bayesian-network/output-1.svg){width=80% fig-align="center"}

```{=tex}
\begin{gather*}
  P({\bf V}) = \prod_{V \in {\bf V}} P(V \mid parents(V))
\end{gather*}
```

:::

::: {.column width="50%" .fragment}

![](figures/junction-tree/output-2.svg){width=80% fig-align="center"}

![](figures/right-arrow-unfilled.svg){width=5% .absolute top=200 left=490}

:::

::::


## Overview

![](figures/jta-overview/output-1-3.svg){width=55% fig-align="center"}


## Observation entry {.smaller visibility="hidden"}

Suppose we observe that $D=0$

. . .

:::: {.columns}

::: {.column width="33%"}

| $D$ | $E$ | $B$ | $\phi$ |
|---|---|---|:---------:|
| 0 | 0 | 0 | 0.25    |
| 0 | 0 | 1 | 0.35    |
| 0 | 1 | 0 | 0.08    |
| 0 | 1 | 1 | 0.16    |
| 1 | 0 | 0 | 0.05    |
| 1 | 0 | 1 | 0.07    |
| 1 | 1 | 0 | 0.00    |
| 1 | 1 | 1 | 0.00    |

:::

::: {.column width="33%" .fragment}

| $D$ | $E$ | $B$ | $\phi$ |
|---|---|---|:---------:|
| 0 | 0 | 0 | 0.25    |
| 0 | 0 | 1 | 0.35    |
| 0 | 1 | 0 | 0.08    |
| 0 | 1 | 1 | 0.16    |
| 1 | 0 | 0 | ~~0.05~~    |
| 1 | 0 | 1 | ~~0.07~~    |
| 1 | 1 | 0 | ~~0.00~~    |
| 1 | 1 | 1 | ~~0.00~~    |

![](figures/right-arrow-unfilled.svg){width=5% .absolute top=250 left=270}

:::

::: {.column width="33%" .fragment}

| $E$ | $B$ | $\phi$ |
|---|---|:---------:|
| 0 | 0 | 0.25    |
| 0 | 1 | 0.35    |
| 1 | 0 | 0.08    |
| 1 | 1 | 0.16    |

![](figures/right-arrow-unfilled.svg){width=5% .absolute top=250 left=615}

:::

::::


## Overview

![](figures/jta-overview/output-1-4.svg){width=55% fig-align="center"}


## Message passing {.smaller}

![](figures/message-passing/output-00.svg){width=50% fig-align="center"}

\begin{equation}
  \phi_{{\bf X} \rightarrow {\bf Y}} =
  \sum_{{\bf X} \setminus {\bf Y}} \psi_{{\bf X}} \prod_{{\bf N} \in \mathcal{N}_{{\bf X} \setminus {\bf Y}}} \phi_{{\bf N} \rightarrow {\bf X}}
\end{equation}

::: {.notes}
  - Represents the local computations that are necessary to spread each
    cluster's information with every other cluster in the graph.
  - Designate an arbitrary cluster as the *root*.
  - This gives direction to the edges.
  - Two passes: *inward* and *outward*.
    - Inward pass: each cluster passes a message to its *parent*.
    - Backward pass: each cluster passes a message to each of its *children*.
  - A cluster can only pass a message to a neighbor after it has received
    messages from all *other* neighbors.
:::

## Message passing {.smaller}

![](figures/message-passing/output-01.svg){width=50% fig-align="center"}

\begin{equation}
  \phi_{{\bf X} \rightarrow {\bf Y}} =
  \sum_{{\bf X} \setminus {\bf Y}} \psi_{{\bf X}} \prod_{{\bf N} \in \mathcal{N}_{{\bf X} \setminus {\bf Y}}} \phi_{{\bf N} \rightarrow {\bf X}}
\end{equation}


## Message passing {.smaller}

![](figures/message-passing/output-02.svg){width=50% fig-align="center"}

\begin{equation}
  \phi_{{\bf X} \rightarrow {\bf Y}} =
  \sum_{{\bf X} \setminus {\bf Y}} \psi_{{\bf X}} \prod_{{\bf N} \in \mathcal{N}_{{\bf X} \setminus {\bf Y}}} \phi_{{\bf N} \rightarrow {\bf X}}
\end{equation}


## Message passing {.smaller}

![](figures/message-passing/output-05.svg){width=50% fig-align="center"}

\begin{equation}
  \phi_{{\bf X} \rightarrow {\bf Y}} =
  \sum_{{\bf X} \setminus {\bf Y}} \psi_{{\bf X}} \prod_{{\bf N} \in \mathcal{N}_{{\bf X} \setminus {\bf Y}}} \phi_{{\bf N} \rightarrow {\bf X}}
\end{equation}


## Message passing {.smaller}

![](figures/message-passing/output-06.svg){width=50% fig-align="center"}

\begin{equation}
  \phi_{{\bf X} \rightarrow {\bf Y}} =
  \sum_{{\bf X} \setminus {\bf Y}} \psi_{{\bf X}} \prod_{{\bf N} \in \mathcal{N}_{{\bf X} \setminus {\bf Y}}} \phi_{{\bf N} \rightarrow {\bf X}}
\end{equation}


## Message passing {.smaller}

![](figures/message-passing/output-07.svg){width=50% fig-align="center"}

\begin{equation}
  \phi_{{\bf X} \rightarrow {\bf Y}} =
  \sum_{{\bf X} \setminus {\bf Y}} \psi_{{\bf X}} \prod_{{\bf N} \in \mathcal{N}_{{\bf X} \setminus {\bf Y}}} \phi_{{\bf N} \rightarrow {\bf X}}
\end{equation}



## Message passing {.smaller}

![](figures/message-passing/output-10.svg){width=50% fig-align="center"}

\begin{equation}
  \phi_{{\bf X} \rightarrow {\bf Y}} =
  \sum_{{\bf X} \setminus {\bf Y}} \psi_{{\bf X}} \prod_{{\bf N} \in \mathcal{N}_{{\bf X} \setminus {\bf Y}}} \phi_{{\bf N} \rightarrow {\bf X}}
\end{equation}

::: {.notes}
Mention that some of these messages can be computed at compile time.
:::


## Overview 

![](figures/jta-overview/output-1-5.svg){width=55% fig-align="center"}

## Marginalization {.smaller}

::: {.notes}
- We marginalize each variable of interest from a sepset or cluster that contains it.
- Suppose we are interested in variables $A$, $B$, and $C$.
- To marginalize a variable from a \textit{cluster} we perform a product
  between the incoming messages and the cluster potential, and sum out all
  other variables.
- To marginalize a variable from a \textit{sepset} we perform a product between
  the two messages incoming messages and sum out all other variables.
:::

![](figures/marginalization/output-1.svg){width=60% fig-align="center"}


## Marginalization {.smaller}

![](figures/marginalization/output-2.svg){width=60% fig-align="center"}

\begin{gather*}
p(B, A=a, D=d) = \sum_{L} ~~~~~~~ \times
\end{gather*}

![](figures/circled-messages/6.svg){width=5% .absolute top=465 left=665}
![](figures/circled-messages/5.svg){width=5% .absolute top=485 left=735}


## Marginalization {.smaller visibility="hidden"}

![](figures/marginalization/output-2.svg){width=60% fig-align="center"}

\begin{gather*}
p(L, A=a, D=d) = \sum_{B} ~~~~~~~ \times
\end{gather*}

![](figures/circled-messages/6.svg){width=5% .absolute top=465 left=665}
![](figures/circled-messages/5.svg){width=5% .absolute top=485 left=735}


## Marginalization {.smaller visibility="hidden"}

![](figures/marginalization/output-3.svg){width=60% fig-align="center"}

\begin{gather*}
p(T, A=a, D=d) = ~~~~~~~ \times
\end{gather*}

![](figures/circled-messages/1.svg){width=5% .absolute top=480 left=635}
![](figures/circled-messages/9.svg){width=5% .absolute top=460 left=720}


## Overview

![](figures/jta-overview/output-1-6.svg){width=55% fig-align="center"}


## Normalization {.smaller visibility="hidden"}

::: {.notes}
\begin{gather*}
P(V \mid {\bf E=e}) = \frac{P(V, {\bf E=e})}{P({\bf E=e})} = \frac{P(V, {\bf E=e})}{\sum_{V} P(V, {\bf E=e})}
\end{gather*}
:::

:::: {.columns}

::: {.column width="50%"}

| $V$ | $\phi(V, {\bf E} = {\bf e})$ |
|---  |------|
| 0   | 0.25 |
| 1   | 0.05 |
| 2   | 0.15 |

:::

::: {.column width="50%"}

| $V$ | $P(v \mid {\bf E} = {\bf e})$ |
|---  |------|
| 0   | 0.55 |
| 1   | 0.11 |
| 2   | 0.33 |

:::

::::

![](figures/right-arrow-unfilled.svg){width=5% .absolute top=150 left=360}


# JunctionTrees.jl

![](figures/junction-trees-logo-short.png){.absolute top=-10 left=580}


## Features {.smaller}

:::: {.columns}

::: {.column width="45%"}
- Posterior marginal computation of discrete variables given evidence.
- Metaprogramming-based design that separates the algorithm into a *compilation*
  and a *runtime* phase.
- Visualization of junction trees, Bayesian networks and Markov random fields.
:::

::: {.column width="55%"}
![](figures/promedus-34-td.svg){width=100% fig-align="center"}
:::

::::


## Runtime performance comparison

![](figures/benchmarks/merlin-vs-junctiontrees.svg){width=100% fig-align="center"}


## Compile-time vs runtime {auto-animate=true}

![](figures/jta-overview/output-2.svg){width=55% fig-align="center"}


## Compile-time vs runtime {auto-animate=true}

![](figures/jta-overview/output-3.svg){width=55% fig-align="center"}


## Basic usage {.smaller visibility="hidden"}

```{julia}
#| output: false
#| echo: false
Base.show(io::IO, x::Array{Float64}) = print(io, "[...]")
```

**Step 1**: Generate the algorithm

```{julia}
#| output-location: fragment
using JunctionTrees

algo = compile_algo("asia/asia.uai"; uai_evid_filepath="asia/asia.uai.evid")
```

## {.smaller visibility="hidden"}

**Step 2**: Evaluate the algorithm

```{julia}
#| output-location: fragment
eval(algo)
```

. . .

**Step 3**: Run the algorithm

```{julia}
#| output-location: fragment
obsvars, obsvals = JunctionTrees.read_uai_evid_file("asia/asia.uai.evid")
marginals = run_algo(obsvars, obsvals)
```

. . .

These are the *posterior marginals* for each variable.


## {.smaller visibility="hidden"}

You can pass an existing junction tree to generate the algorithm

```{julia}
#| code-line-numbers: "|2"
#| output: false
algo = compile_algo("asia/asia.uai", uai_evid_filepath = "asia/asia.uai.evid",
                    td_filepath = "asia/asia.td")
eval(algo)
marginals = run_algo([1,8], [1,1])
```

. . .

![](figures/benchmarks/minfill-vs-tamaki.svg){width=100% fig-align="center"}


## Metaprogramming-based framework

![](figures/metaprogramming-based-framework/output.svg){width=90% fig-align="center"}


## Partial evaluation {.smaller}


![](figures/partial-evaluation-messages/output-1.svg){width=50% fig-align="center"}

::: footer
*Partial Evaluation in Junction Trees* by M. ROA VILLESCAS et. al. accepted in DSD/SEAA 2022.
:::


## Partial evaluation {.smaller}

![](figures/partial-evaluation-messages/output-2.svg){width=50% fig-align="center"}

::: footer
*Partial Evaluation in Junction Trees* by M. ROA VILLESCAS et. al. accepted in DSD/SEAA 2022.
:::


## Partial evaluation 

![](figures/benchmarks/partial-evaluation.svg){width=100% fig-align="center"}

::: footer
*Partial Evaluation in Junction Trees* by M. ROA VILLESCAS et. al. accepted in DSD/SEAA 2022.
:::


# The end


# Questions?
